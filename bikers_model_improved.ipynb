{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVs3XwRMMzXUb6BZIQ0FdC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EnmaSantos/CSE450/blob/main/bikers_model_improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWK-F-a0v7iy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset\n",
        "data_url = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv\"\n",
        "data = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "5ir6ytpTwBB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the mini holdout dataset\n",
        "mini_holdout_url = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv\"\n",
        "mini_holdout = pd.read_csv(mini_holdout_url)"
      ],
      "metadata": {
        "id": "AqWPRuatwC5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the 'count' column to the training dataset\n",
        "data['count'] = data['casual'] + data['registered']"
      ],
      "metadata": {
        "id": "6rZgXpHxwEzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'casual' and 'registered' columns\n",
        "data = data.drop(columns=['casual', 'registered'])\n",
        "\n",
        "# Convert 'dteday' to datetime format\n",
        "data['dteday'] = pd.to_datetime(data['dteday'])\n",
        "mini_holdout['dteday'] = pd.to_datetime(mini_holdout['dteday'])\n",
        "\n",
        "# Function to engineer features\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Extract temporal features\n",
        "    df['year'] = df['dteday'].dt.year\n",
        "    df['month'] = df['dteday'].dt.month\n",
        "    df['day'] = df['dteday'].dt.day\n",
        "    df['day_of_week'] = df['dteday'].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Cyclical encoding for hour (captures time of day patterns)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hr'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hr'] / 24)\n",
        "\n",
        "    # Cyclical encoding for month (captures seasonal patterns)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "    # Cyclical encoding for day of week (captures weekly patterns)\n",
        "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "\n",
        "    # Interaction features (capture combined effects)\n",
        "    df['temp_hum'] = df['temp_c'] * df['hum']  # Hot and humid days impact differently\n",
        "    df['temp_windspeed'] = df['temp_c'] * df['windspeed']  # Wind chill effect\n",
        "    df['hum_windspeed'] = df['hum'] * df['windspeed']  # Combined weather impact\n",
        "\n",
        "    # Enhanced holiday/workingday features\n",
        "    df['special_day'] = ((df['holiday'] == 1) | (df['is_weekend'] == 1)).astype(int)\n",
        "\n",
        "    # Create hour groups with unique labels\n",
        "    df['hour_group'] = pd.cut(df['hr'],\n",
        "                              bins=[-1, 5, 11, 16, 20, 24],\n",
        "                              labels=['early_morning', 'morning', 'afternoon', 'evening', 'night'])\n",
        "\n",
        "    # Create \"rush hour\" feature (typical commuting times)\n",
        "    df['rush_hour'] = ((df['hr'] >= 7) & (df['hr'] <= 9) |\n",
        "                       (df['hr'] >= 16) & (df['hr'] <= 18)).astype(int)\n",
        "\n",
        "    # Weather and holiday interaction (bad weather on holidays has different impact)\n",
        "    df['weather_holiday'] = df['weathersit'] * df['holiday']\n",
        "\n",
        "    # COVID-19 periods (since dataset spans 2011-2023)\n",
        "    df['pre_covid'] = (df['dteday'] < '2020-03-01').astype(int)\n",
        "    df['early_covid'] = ((df['dteday'] >= '2020-03-01') & (df['dteday'] < '2021-06-01')).astype(int)\n",
        "    df['late_covid'] = ((df['dteday'] >= '2021-06-01') & (df['dteday'] < '2022-04-01')).astype(int)\n",
        "    df['post_covid'] = (df['dteday'] >= '2022-04-01').astype(int)\n",
        "\n",
        "    # Long-term trend capture (years since start of data)\n",
        "    df['years_since_start'] = (df['dteday'].dt.year - 2011) + (df['dteday'].dt.month - 1)/12\n",
        "\n",
        "    # Interaction between COVID periods and other features\n",
        "    df['covid_weather'] = ((df['early_covid'] == 1) | (df['late_covid'] == 1)) * df['weathersit']\n",
        "    df['covid_weekend'] = ((df['early_covid'] == 1) | (df['late_covid'] == 1)) * df['is_weekend']\n",
        "\n",
        "    # Create one-hot encoding for hour_group\n",
        "    hour_group_dummies = pd.get_dummies(df['hour_group'], prefix='hour_group')\n",
        "    df = pd.concat([df.drop(columns=['hour_group']), hour_group_dummies], axis=1)\n",
        "\n",
        "    # Drop 'dteday' after extracting all features\n",
        "    df = df.drop(columns=['dteday'])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "fxclaopHwGtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature engineering\n",
        "data = engineer_features(data)\n",
        "mini_holdout = engineer_features(mini_holdout)\n",
        "\n",
        "# Visualize the relationship between hour and count\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x=data['hr'], y=data['count'])\n",
        "plt.title('Bike Rentals by Hour')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Rentals')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the relationship between season and count\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x=data['season'], y=data['count'])\n",
        "plt.title('Bike Rentals by Season')\n",
        "plt.xlabel('Season (1:winter, 2:spring, 3:summer, 4:fall)')\n",
        "plt.ylabel('Number of Rentals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ap3VfWD2wMuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical features\n",
        "categorical_features = ['season', 'weathersit', 'holiday', 'workingday']\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "encoded_features = encoder.fit_transform(data[categorical_features])\n",
        "encoded_columns = encoder.get_feature_names_out(categorical_features)\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoded_columns)\n",
        "data = pd.concat([data.drop(columns=categorical_features), encoded_df], axis=1)\n",
        "\n",
        "# Identify numerical features (exclude 'count' and any other non-feature columns)\n",
        "numerical_features = [col for col in data.columns\n",
        "                      if col not in ['count', 'instant']\n",
        "                      and not col.startswith('season_')\n",
        "                      and not col.startswith('weathersit_')\n",
        "                      and not col.startswith('holiday_')\n",
        "                      and not col.startswith('workingday_')]\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns=['count'])\n",
        "y = data['count']\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training a gradient boosting model first to determine feature importance...\")\n",
        "# Train a gradient boosting model to identify important features\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': gb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"Top 15 most important features:\")\n",
        "print(feature_importances.head(15))\n",
        "\n",
        "# Visualize feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(15))\n",
        "plt.title('Top 15 Feature Importances')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xq3iQEIEwRyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNow training the neural network model...\")\n",
        "# Build the neural network with attention to the most important features\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='relu')  # ReLU activation ensures non-negative predictions\n",
        "])\n",
        "\n",
        "# Compile the model with a better optimizer\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "# Callbacks for early stopping and learning rate scheduling\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.0001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,  # Increased epochs but using early stopping\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Process the holdout dataset\n",
        "# One-hot encode categorical features\n",
        "encoded_features_holdout = encoder.transform(mini_holdout[categorical_features])\n",
        "encoded_df_holdout = pd.DataFrame(encoded_features_holdout, columns=encoded_columns)\n",
        "mini_holdout = pd.concat([mini_holdout.drop(columns=categorical_features), encoded_df_holdout], axis=1)\n",
        "\n",
        "# Ensure mini_holdout has the same columns as X_train\n",
        "for col in X_train.columns:\n",
        "    if col not in mini_holdout.columns:\n",
        "        mini_holdout[col] = 0  # Add missing columns with default value\n",
        "\n",
        "# Reorder columns to match X_train\n",
        "mini_holdout = mini_holdout[X_train.columns]\n",
        "\n",
        "# Normalize numerical features\n",
        "mini_holdout[numerical_features] = scaler.transform(mini_holdout[numerical_features])"
      ],
      "metadata": {
        "id": "2_dEr3yXweat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "predicted_counts = model.predict(mini_holdout).flatten()\n",
        "\n",
        "# Post-processing to ensure predictions are positive integers\n",
        "predicted_counts = np.maximum(0, predicted_counts)  # Ensure non-negative\n",
        "predicted_counts = predicted_counts.round(0).astype(int)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results = pd.DataFrame({'count': predicted_counts})\n",
        "results.to_csv(\"team8-bike-rental-predictions.csv\", index=False)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vSv60yOHwg1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output model summary and evaluation metrics\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nTraining completed with early stopping at epoch:\",\n",
        "      len(history.history['loss']))\n",
        "\n",
        "print(\"\\nValidation Loss:\", history.history['val_loss'][-1])\n",
        "print(\"Validation MAE:\", history.history['val_mae'][-1])\n",
        "\n",
        "print(\"\\nChecking for negative or extreme predictions:\")\n",
        "print(\"Min prediction:\", min(predicted_counts))\n",
        "print(\"Max prediction:\", max(predicted_counts))\n",
        "print(\"Average prediction:\", sum(predicted_counts)/len(predicted_counts))\n",
        "\n",
        "# Additional insights\n",
        "print(\"\\nDistribution of predictions:\")\n",
        "bins = [0, 50, 100, 200, 500, 1000, float('inf')]\n",
        "bin_labels = ['0-50', '51-100', '101-200', '201-500', '501-1000', '1000+']\n",
        "prediction_distribution = pd.cut(predicted_counts, bins=bins, labels=bin_labels)\n",
        "print(pd.value_counts(prediction_distribution, normalize=True).sort_index() * 100)\n",
        "\n",
        "# Check if we have good variety in our predictions (not just the same value repeated)\n",
        "unique_predictions = len(set(predicted_counts))\n",
        "print(f\"\\nNumber of unique prediction values: {unique_predictions} out of {len(predicted_counts)} predictions\")\n",
        "print(f\"Percentage of unique values: {unique_predictions/len(predicted_counts)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "xce_F-BYwjHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}